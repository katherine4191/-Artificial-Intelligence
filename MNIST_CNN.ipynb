{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: gpu\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: gpu\") if torch.cuda.is_available() else print(\"device: cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypter parameter 설정\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "display_step = 10\n",
    "batch_size = 32\n",
    "dropout_rate = .2\n",
    "\n",
    "activation = nn.ReLU()\n",
    "max_pool = nn.MaxPool2d(2,2) # kerel size, stride size, padding size \n",
    "drop_out = nn.Dropout2d(dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = dataset.MNIST(\"./\", train = True, transform = transforms.ToTensor(), target_transform = None, download = True)\n",
    "test_data = dataset.MNIST(\"./\", train = False, transform = transforms.ToTensor(), target_transform = None, download = True)\n",
    "\n",
    "# pre-process (batch, shuffle)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-659b75651d94>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "# model 작성\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), # 32 28 28\n",
    "            activation,\n",
    "            max_pool, # 32 14 14\n",
    "            nn.Conv2d(32,64,3,padding=1), # 64 14 14\n",
    "            drop_out,\n",
    "            nn.Conv2d(64,64,3,padding=1), # 64 14 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            activation, \n",
    "            max_pool # 64 7 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 50),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        extracted_feature = self.feature_extraction(x) # [32, 64, 3, 3]\n",
    "        flatten = extracted_feature.view(batch_size, -1) # [32, 576 (64 * 3 * 3)]\n",
    "        result = self.classifier(flatten)\n",
    "        return result\n",
    "\n",
    "model = CNN()\n",
    "model.apply(init_weights)\n",
    "model=model.to(device)\n",
    "model.train()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.99) ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch lr: [0.0009801]\n",
      "0 epoch loss: 0.0037397986743599176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epoch lr: [0.0008863848717161291]\n",
      "10 epoch loss: 0.0012666613329201937\n",
      "20 epoch lr: [0.0008016305895390457]\n",
      "20 epoch loss: 4.35749716416467e-05\n"
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    scheduler.step() ###\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch lr: {}'.format(i,scheduler.get_lr())) ###\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        loss_array.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.9926882982254028\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, [data, label] in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = model.forward(data)\n",
    "        _, prediction_index = torch.max(output, 1)\n",
    "        \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(label)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (prediction_index == label).sum().float()\n",
    "\n",
    "    print(\"Accuracy of the model: {}\".format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "# batch size 32 -> 0.97996...\n",
    "# batch size 64 -> 0.97015... -> 32로 설정\n",
    "# nn.BatchNorm2d(64) -> 0.98717....\n",
    "# nn.BatchNorm2d(32) -> 0.98637... -> delete\n",
    "# optim.SGD -> optim.Adam -> 0.989...\n",
    "# linear fuction 추가 -> 0.9905.. -> delete\n",
    "# lr scheduler 추가 -> 0.9926..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
